{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "LjRqJ7sS5vsQ",
        "2KHNCw_VebTb",
        "5bGRRrGK46pc",
        "bD4Bn10C6YWr"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZzR2-U43r4"
      },
      "source": [
        "# **Leveraging BERT for Natural Language Understanding of Domain-Specific Knowledge**\n",
        "\n",
        "This notebook is based on the paper __BERT for Joint Intent Classification and Slot Filling__ by Chen et al. (2019), --> https://arxiv.org/abs/1902.10909\n",
        "It is based on Shawon Ashraf's notebook, available here --> https://github.com/ShawonAshraf/nlu-jointbert-dl2021.\n",
        "\n",
        "This notebook is the running code for the paper Leveraging BERT for Natural Language Understanding of Domain-Specific Knowledge, by V.I. Iga and G.C. Silaghi, available here --> https://github.com/IonutIga/Domain-Specific-NLU-BERT/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **In order for this notebook to run properly**, load the datasets available at --> https://github.com/IonutIga/Domain-Specific-NLU-BERT, from the Datasets folder.\n",
        "You can train ATIS, SNIPS and/or the custom generated dataset (generated using the Dialogue Simulator available here --> https://github.com/IonutIga/Dialogue-Simulator)."
      ],
      "metadata": {
        "id": "h6mfNrlLdflj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpJIziCh6oy8"
      },
      "source": [
        "## Dataset format\n",
        "\n",
        "Data is of the following format\n",
        "````json5\n",
        "{\n",
        "  \"text\": \"\",\n",
        "  \"positions\": [{}],\n",
        "  \"slots\": [{}],\n",
        "  \"intent\": \"\"\n",
        "}\n",
        "````\n",
        "\n",
        "We will be using `text` as the input and `slots` and `intent` as lables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjRqJ7sS5vsQ"
      },
      "source": [
        "## Install and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OO2HRXgV5HA"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install seqeval\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "from datasets import load_metric\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions"
      ],
      "metadata": {
        "id": "2KHNCw_VebTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate de start and end index of a word in a phrase\n",
        "def entityDetails(phrase, entity):\n",
        "  startIndex = phrase.find(entity)\n",
        "  endIndex = startIndex + len(entity) - 1\n",
        "  return startIndex, endIndex\n",
        "\n",
        "# read a random line from a file\n",
        "def read_lines(file):\n",
        "  f = open(file)\n",
        "  lines = f.read().splitlines()\n",
        "  f.close()\n",
        "  return lines"
      ],
      "metadata": {
        "id": "b8OeGZ1peZt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert text format into JSON**. ATIS and SNIPS are text format phrases, while our generated dataset stores information in JSON format. To train our model with the two datasets, we design a function that converts plain texts and intent and slots labeling into JSON objects. Unfortunately, there is a loss of information during the process, as some texts may have multiple same slots in the same text, but JSON needs unqique keys to store information, therefore one slot can only appear once. Although this caveat, ATIS and SNIPS texts have, most of the times, only one appearance per slot per text, therefore we do not lose much information."
      ],
      "metadata": {
        "id": "uuKXogUJmFyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the name of the resulted JSON file for out_file\n",
        "\n",
        "def convert_text_format_to_JSON(labels, seqin, seqout, out_file):\n",
        "\n",
        "  annotated_phrases = {}\n",
        "  annotated_phrase = {'text' : '',\n",
        "                      'slots' : {},\n",
        "                      'positions':{},\n",
        "                      'intent': ''}\n",
        "  for i in range(len(seqin)):\n",
        "    annotated_phrase['text'] = seqin[i]\n",
        "    annotated_phrase['intent'] = labels[i]\n",
        "    tok_seqout = seqout[i].split(' ')\n",
        "    tok_seqin = seqin[i].split(' ')\n",
        "    complete_word = ''\n",
        "    complete_label = ''\n",
        "    for k, v in zip(tok_seqout, tok_seqin):\n",
        "      if k != 'O':\n",
        "        if k.startswith('B'):\n",
        "          if complete_word:\n",
        "            si, ei = entityDetails(seqin[i], complete_word)\n",
        "            annotated_phrase['slots'][complete_label] = complete_word\n",
        "            annotated_phrase['positions'][complete_label] = [si, ei]\n",
        "            complete_word = v\n",
        "            complete_label = k\n",
        "\n",
        "          else:\n",
        "            complete_word += v\n",
        "            complete_label = k\n",
        "        elif k.startswith('I'):\n",
        "          if complete_word != '':\n",
        "            complete_word += f' {v}'\n",
        "      elif complete_word:\n",
        "        si, ei = entityDetails(seqin[i], complete_word)\n",
        "        annotated_phrase['slots'][complete_label] = complete_word\n",
        "        annotated_phrase['positions'][complete_label] = [si, ei]\n",
        "        complete_label = ''\n",
        "        complete_word = ''\n",
        "    if complete_word:\n",
        "      si, ei = entityDetails(seqin[i], complete_word)\n",
        "      annotated_phrase['slots'][complete_label] = complete_word\n",
        "      annotated_phrase['positions'][complete_label] = [si, ei]\n",
        "    annotated_phrases[i] = annotated_phrase\n",
        "    annotated_phrase = {'text' : '',\n",
        "                      'slots' : {},\n",
        "                      'positions':{},\n",
        "                      'intent': ''}\n",
        "  out_file = open(out_file, 'w')\n",
        "  json.dump(annotated_phrases, out_file, indent = 4)\n",
        "  out_file.close()\n",
        "  return None"
      ],
      "metadata": {
        "id": "3jT7ekOIl6Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RawData(object):\n",
        "    def __init__(self, id, intent, positions, slots, text):\n",
        "        self.id = id\n",
        "        self.intent = intent\n",
        "        self.positions = positions\n",
        "        self.slots = slots\n",
        "        self.text = text\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(json.dumps(self.__dict__, indent=2))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "reads json from data file\n",
        "returns a list containing DataInstance objects\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "m2yLI42Wehpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_train_json_file(filename):\n",
        "    if os.path.exists(filename):\n",
        "        intents = []\n",
        "\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
        "            data = json.load(json_file)\n",
        "\n",
        "            for k in data.keys():\n",
        "                intent = data[k][\"intent\"]\n",
        "                positions = data[k][\"positions\"]\n",
        "                slots = data[k][\"slots\"]\n",
        "                text = data[k][\"text\"]\n",
        "\n",
        "                temp = RawData(k, intent, positions, slots, text)\n",
        "                intents.append(temp)\n",
        "\n",
        "        return intents\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No file found with that path!\")"
      ],
      "metadata": {
        "id": "Vl7KXC036cKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode intents into tensors\n",
        "def encode_intents(intents, intent_map):\n",
        "    encoded = []\n",
        "    for i in intents:\n",
        "        encoded.append(intent_map[i])\n",
        "    # convert to tf tensor\n",
        "    return encoded, tf.convert_to_tensor(encoded, dtype=\"int32\")"
      ],
      "metadata": {
        "id": "JFGr7vU_fu35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gets slot name from its values\n",
        "def get_slot_from_word(word, slot_dict):\n",
        "    for slot_label,value in slot_dict.items():\n",
        "        if word in value.split():\n",
        "            return slot_label\n",
        "    return None"
      ],
      "metadata": {
        "id": "d6ze84BYhCVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize input by a pattern, removing unnecessary spaces\n",
        "def tokenize(pattern, text):\n",
        "  final_text = []\n",
        "  tokens = re.split(pattern,text)\n",
        "  for t in tokens:\n",
        "    if t not in ['', ' ']:\n",
        "      final_text.append(t)\n",
        "  return final_text"
      ],
      "metadata": {
        "id": "0HP3eFtzhl_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to encode slots from each text\n",
        "def encode_slots(all_slots, all_texts,\n",
        "                 tokenizer, slot_map, max_len):\n",
        "    encoded_slots = np.zeros(shape=(len(all_texts), max_len), dtype=np.int32)\n",
        "\n",
        "    for idx, text in enumerate(all_texts):\n",
        "        enc = [] # for this idx, to be added at the end to encoded_slots\n",
        "\n",
        "        # slot names for this idx\n",
        "        slot_names = all_slots[idx]\n",
        "\n",
        "        # raw word tokens\n",
        "        # not using bert for this block because bert uses\n",
        "        # a wordpiece tokenizer which will make\n",
        "        # the slot label to word mapping\n",
        "        # difficult\n",
        "        raw_tokens = tokenize('( |,)', text)\n",
        "\n",
        "        # words or slot_values associated with a certain\n",
        "        # slot_name are contained in the values of the\n",
        "        # dict slots_names\n",
        "        # now this becomes a two way lookup\n",
        "        # first we check if a word belongs to any\n",
        "        # slot label or not and then we add the value from\n",
        "        # slot map to encoded for that word\n",
        "        for rt in raw_tokens:\n",
        "            # use bert tokenizer\n",
        "            # to get wordpiece tokens\n",
        "            bert_tokens = tokenizer.tokenize(rt)\n",
        "\n",
        "            # find the slot name for a token\n",
        "            rt_slot_name = get_slot_from_word(rt, slot_names)\n",
        "            if rt_slot_name is not None:\n",
        "                # fill with the slot_map value for all bert tokens for rt\n",
        "                enc.append(slot_map[rt_slot_name])\n",
        "                enc.extend([slot_map[rt_slot_name]] * (len(bert_tokens) - 1))\n",
        "            else:\n",
        "                # rt is not associated with any slot name\n",
        "                enc.append(0)\n",
        "                enc.extend([0] * (len(bert_tokens) - 1))\n",
        "\n",
        "        # now add to encoded_slots\n",
        "        # ignore the first and the last elements\n",
        "        # in encoded text as they're special chars\n",
        "        encoded_slots[idx, 1:len(enc)+1] = enc\n",
        "\n",
        "    return encoded_slots"
      ],
      "metadata": {
        "id": "7OK8lhgcicbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifier definiton**:\n",
        "The model uses BERT as a base transformer layer. On top of it, a Dropout layer is added. Finally, two Dense layers, one for Intent Detection and the other for Slot Filling, are placed at the top of the model. It returns logit values."
      ],
      "metadata": {
        "id": "AjOqjf-8i04o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n",
        "                 model_name=model_name, dropout_prob=0.1):\n",
        "        super().__init__(name=\"joint_intent_slot\")\n",
        "        self.bert = TFBertModel.from_pretrained(model_name)\n",
        "        self.dropout = Dropout(dropout_prob)\n",
        "        self.intent_classifier = Dense(intent_num_labels,\n",
        "                                       name=\"intent_classifier\")\n",
        "        self.slot_classifier = Dense(slot_num_labels,\n",
        "                                     name=\"slot_classifier\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # two outputs from BERT\n",
        "        trained_bert = self.bert(inputs, **kwargs)\n",
        "        pooled_output = trained_bert.pooler_output\n",
        "        sequence_output = trained_bert.last_hidden_state\n",
        "\n",
        "        # sequence_output will be used for slot_filling / classification\n",
        "        sequence_output = self.dropout(sequence_output,\n",
        "                                       training=kwargs.get(\"training\", False))\n",
        "        slot_logits = self.slot_classifier(sequence_output)\n",
        "\n",
        "        # pooled_output for intent classification\n",
        "        pooled_output = self.dropout(pooled_output,\n",
        "                                     training=kwargs.get(\"training\", False))\n",
        "        intent_logits = self.intent_classifier(pooled_output)\n",
        "\n",
        "        return slot_logits, intent_logits"
      ],
      "metadata": {
        "id": "O09wxEngizER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The NLU pipeline** is used to predict intent and slots from a specific text."
      ],
      "metadata": {
        "id": "fp7O5PYmjsTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
        "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
        "    outputs = model(inputs)\n",
        "    slot_logits, intent_logits = outputs\n",
        "\n",
        "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
        "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
        "\n",
        "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
        "\n",
        "    out_dict = {}\n",
        "    # get all slot names and add to out_dict as keys\n",
        "    predicted_slots = set([slot_names[s] for s in slot_ids if s != 0])\n",
        "    for ps in predicted_slots:\n",
        "      out_dict[ps] = []\n",
        "\n",
        "    tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
        "\n",
        "    for token, slot_id in zip(tokens, slot_ids):\n",
        "        # add all to out_dict\n",
        "        slot_name = slot_names[slot_id]\n",
        "\n",
        "        if slot_name == \"O\":\n",
        "            continue\n",
        "\n",
        "        # collect tokens\n",
        "        collected_tokens = [token]\n",
        "        idx = tokens.index(token)\n",
        "\n",
        "        # see if it starts with ##\n",
        "        # then it belongs to the previous token\n",
        "        if token.startswith(\"##\"):\n",
        "          # check if the token already exists or not\n",
        "          if tokens[idx - 1] not in out_dict[slot_name]:\n",
        "            collected_tokens.insert(0, tokens[idx - 1])\n",
        "\n",
        "        # add collected tokens to slots\n",
        "        out_dict[slot_name].extend(collected_tokens)\n",
        "    # process out_dict\n",
        "    for slot_name in out_dict:\n",
        "        tokens = out_dict[slot_name]\n",
        "        slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
        "        info[\"slots\"][slot_name] = slot_value.strip()\n",
        "\n",
        "    return info, slot_ids, intent_id"
      ],
      "metadata": {
        "id": "yR1sFlvljpy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_VahZ-O7JXU"
      },
      "source": [
        "Load Tokenizer from transformers\n",
        "\n",
        "We will use a pretrained bert model `bert-base-cased` for both Tokenizer and our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA8sh4pLVT84"
      },
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ATIS dataset**"
      ],
      "metadata": {
        "id": "5bGRRrGK46pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download train data for ATIS"
      ],
      "metadata": {
        "id": "UM9FKNy04mxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/train/label\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/train/seq.in\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/train/seq.out"
      ],
      "metadata": {
        "id": "SMDhX8ziXY4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download test data for ATIS"
      ],
      "metadata": {
        "id": "Ms8D8jB3khTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/test/label\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/test/seq.in\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/atis/test/seq.out"
      ],
      "metadata": {
        "id": "ibFZ8OKykhdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets into one, so all the preprocessing is done at once\n",
        "labels = read_lines(\"label\") + read_lines('label.1')\n",
        "seqin = read_lines('seq.in') + read_lines('seq.in.1')\n",
        "seqout = read_lines('seq.out') + read_lines('seq.out.1')"
      ],
      "metadata": {
        "id": "ewNCGosha5p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_text_format_to_JSON(labels, seqin, seqout, 'atis.json')"
      ],
      "metadata": {
        "id": "dOOzEPUyfHs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYLzmWhVT84"
      },
      "source": [
        "# read from json file\n",
        "train_data = read_train_json_file(\"atis.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hgnD-4G3VT84"
      },
      "source": [
        "example = train_data[0]\n",
        "example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "DSptdUVIzq6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFs80Rg7jj4"
      },
      "source": [
        "Encode texts from the dataset.\n",
        "We have to encode the texts using the tokenizer to create tensors for training the classifier.\n",
        "Training set for ATIS from 0 to 4477 and test set from 4478 to end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpUnhqA2VT84"
      },
      "source": [
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "\n",
        "\n",
        "def encode_texts(tokenizer, texts):\n",
        "    return tokenizer(texts[:4477], padding=True, truncation=True, return_tensors=\"tf\"), tokenizer(texts[4478:], padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "texts = [d.text for d in train_data]\n",
        "tds, test_tds = encode_texts(tokenizer, texts)\n",
        "tds.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjaIbB7vVT84"
      },
      "source": [
        "encoded_texts = tds\n",
        "encoded_texts_test = test_tds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "P1eTfXPQVT84"
      },
      "source": [
        "# get the unique list of intents' names\n",
        "intents = [d.intent for d in train_data]\n",
        "intent_names = list(set(intents))\n",
        "intent_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDCv98XHVT84"
      },
      "source": [
        "# map intents' names to indexes, which is going to be used by the model to assign predictions\n",
        "intent_map = dict() # index -> intent\n",
        "for idx, ui in enumerate(intent_names):\n",
        "    intent_map[ui] = idx\n",
        "intent_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse intents' names with ids, to use when predicting for converting an ID into natural language\n",
        "id_to_intent_name = {v: k for k, v in intent_map.items()}"
      ],
      "metadata": {
        "id": "4mKAU8DxYMsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-PxXNWcVT84"
      },
      "source": [
        "true_intents, encoded_intents = encode_intents(intents, intent_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0i00AW8gjA"
      },
      "source": [
        "Slots\n",
        "\n",
        "To padd all the texts to the same length, the tokenizer will use special characters. To handle those we need to add O to slots_names. It can be some other symbol as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNW91LSgVT84"
      },
      "source": [
        "# encode slots\n",
        "slot_names = set()\n",
        "for td in train_data:\n",
        "    slots = td.slots\n",
        "    for slot in slots:\n",
        "        slot_names.add(slot)\n",
        "slot_names = list(slot_names)\n",
        "slot_names.insert(0, \"O\")\n",
        "slot_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fv7aVEoVT84"
      },
      "source": [
        "# map slots' names to indexes, which is going to be used by the model to assign predictions\n",
        "slot_map = dict() # slot -> index\n",
        "for idx, us in enumerate(slot_names):\n",
        "    slot_map[us] = idx\n",
        "slot_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse slots' names with ids, to use when predicting for converting an ID into natural language\n",
        "id_to_slot_name = {v: k for k, v in slot_map.items()}"
      ],
      "metadata": {
        "id": "WNL1fWtQYnSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62lsuEZoVT84"
      },
      "source": [
        "# test to see if the slots are aligned with the texts, by getting the slot name for a token within a text\n",
        "print(train_data[0].text)\n",
        "print(train_data[0].slots)\n",
        "print(\"slot_name for baltimore is : \", get_slot_from_word(\"baltimore\", train_data[0].slots))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VUnAC8-VT84"
      },
      "source": [
        "# find the max encoded test length\n",
        "# tokenizer pads all texts to same length anyway so\n",
        "# just get the length of the first one's input_ids\n",
        "max_len = len(encoded_texts[\"input_ids\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Hj-St4VT84"
      },
      "source": [
        "# get all the slots and texts, to encode the slots\n",
        "all_slots = [td.slots for td in train_data]\n",
        "all_texts = [td.text for td in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIEsMzT8VT84"
      },
      "source": [
        "encoded_slots = encode_slots(all_slots, all_texts, tokenizer, slot_map, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMpHQUuwVT84"
      },
      "source": [
        "# define the model\n",
        "joint_model_atis = JointIntentAndSlotFillingModel(\n",
        "    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaI_qVxI-Xo4"
      },
      "source": [
        "Hyperparams, Optimizer and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZeZf3JVT84"
      },
      "source": [
        "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
        "\n",
        "# two outputs, one for slots, another for intents\n",
        "# we have to fine tune for both\n",
        "losses = [SparseCategoricalCrossentropy(from_logits=True),\n",
        "          SparseCategoricalCrossentropy(from_logits=True)]\n",
        "\n",
        "metrics = [SparseCategoricalAccuracy(\"accuracy\")]\n",
        "# compile model\n",
        "joint_model_atis.compile(optimizer=opt, loss=losses, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogJCgyMG-o4A"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uae9vd77VT84"
      },
      "source": [
        "x = {\"input_ids\": encoded_texts[\"input_ids\"], \"token_type_ids\": encoded_texts[\"token_type_ids\"],  \"attention_mask\": encoded_texts[\"attention_mask\"]}\n",
        "\n",
        "history = joint_model_atis.fit(\n",
        "    x, (encoded_slots[:4477], encoded_intents[:4477]), epochs=2, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSkQ6mZ-sMg"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slot labels for the text below\n",
        "#O O O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O O O O B-stoploc.city_name I-stoploc.city_name"
      ],
      "metadata": {
        "id": "U6PnsDGbkFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzuZBVpu_K9Q"
      },
      "source": [
        "nlu(\"i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\", tokenizer, joint_model_atis,\n",
        "    intent_names, slot_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slot labels for the text below\n",
        "#O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name"
      ],
      "metadata": {
        "id": "wnk-nlpeUyR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXSXyLFV_fAE"
      },
      "source": [
        "nlu(\"on april first i need a flight going from phoenix to san diego\", tokenizer, joint_model_atis,\n",
        "    slot_names, slot_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING**"
      ],
      "metadata": {
        "id": "kCo29x17kTsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict intent and slots for each text from test dataset\n",
        "\n",
        "results = []\n",
        "slots_ids = []\n",
        "intent_ids = []\n",
        "for i in tqdm(range(len(all_texts[4478:]))):\n",
        "    res, slot_id, intent_id = nlu(all_texts[i+4478], tokenizer, joint_model_atis, slot_names, slot_names)\n",
        "    results.append(res)\n",
        "    slots_ids.append(slot_id)\n",
        "    intent_ids.append(intent_id)"
      ],
      "metadata": {
        "id": "PqBgokG6VVrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate slot metrics\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(slots_ids)):\n",
        "    for prediction, label in zip(slots_ids, encoded_slots[4478:]):\n",
        "        for predicted_idx, label_idx in zip(prediction, label):\n",
        "            all_predictions.append(id_to_slot_name[predicted_idx])\n",
        "            all_labels.append(id_to_slot_name[label_idx])\n",
        "metrics_to_write = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write"
      ],
      "metadata": {
        "id": "XzPabK31VTAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate intent metrics\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(intent_ids)):\n",
        "        for predicted_idx, label_idx in zip(intent_ids, true_intents[4478:]):\n",
        "            all_predictions.append(id_to_intent_name[predicted_idx])\n",
        "            all_labels.append(id_to_intent_name[label_idx])\n",
        "metrics_to_write_intent = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write_intent"
      ],
      "metadata": {
        "id": "xdLt5ex6v1DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual slots accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "for prediction, label in zip(slots_ids, encoded_slots[4478:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/counter)"
      ],
      "metadata": {
        "id": "o9uciWjG3X-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual overall accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "final_correct = 0\n",
        "i = 4478\n",
        "j = 0\n",
        "test_data_length = 893\n",
        "for prediction, label in zip(slots_ids, encoded_slots[4478:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "  if counter == correct:\n",
        "    print(i,j)\n",
        "    if intent_ids[j] == true_intents[i]:\n",
        "      final_correct +=1\n",
        "  correct = 0\n",
        "  counter = 0\n",
        "  i += 1\n",
        "  j += 1\n",
        "print(final_correct/test_data_length)"
      ],
      "metadata": {
        "id": "_KAtnn4PH90Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual intent accuracy\n",
        "\n",
        "correct = 0\n",
        "for p, l in zip(intent_ids, true_intents[4478:]):\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/test_data_length)"
      ],
      "metadata": {
        "id": "5V9MMSzz2fUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SNIPS dataset**"
      ],
      "metadata": {
        "id": "bD4Bn10C6YWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download train data for SNIPS"
      ],
      "metadata": {
        "id": "P60GLH616YWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/train/label\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/train/seq.in\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/train/seq.out"
      ],
      "metadata": {
        "id": "5UKG41oQ6YWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download test data for SNIPS"
      ],
      "metadata": {
        "id": "RtpJYcc06YWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/test/label\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/test/seq.in\n",
        "!wget https://raw.githubusercontent.com/monologg/JointBERT/master/data/snips/test/seq.out"
      ],
      "metadata": {
        "id": "47p7nZg06YWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the two datasets into one, so all the preprocessing is done at once\n",
        "labels = read_lines(\"label.2\") + read_lines('label.3')\n",
        "seqin = read_lines('seq.in.2') + read_lines('seq.in.3')\n",
        "seqout = read_lines('seq.out.2') + read_lines('seq.out.3')"
      ],
      "metadata": {
        "id": "5LdFtya36YWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_text_format_to_JSON(labels, seqin, seqout,'snips.json')"
      ],
      "metadata": {
        "id": "ZTVG6PY06YW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QVjmMm04HUW"
      },
      "source": [
        "# read from json file\n",
        "train_data = read_train_json_file(\"snips.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xKl1ZfTu4HUY"
      },
      "source": [
        "example = train_data[0]\n",
        "example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode texts from the dataset.\n",
        "We have to encode the texts using the tokenizer to create tensors for training the classifier.\n",
        "Training set for SNIPS from 0 to 13084 and test set from 13085 to end.\n"
      ],
      "metadata": {
        "id": "W4DBVMjOnprc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQKdY0lO4HUd"
      },
      "source": [
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "\n",
        "def encode_texts(tokenizer, texts):\n",
        "    return tokenizer(texts[:13084], padding=True, truncation=True, return_tensors=\"tf\"), tokenizer(texts[13085:], padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "texts = [d.text for d in train_data]\n",
        "tds, test_tds = encode_texts(tokenizer, texts)\n",
        "tds.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTSZW_qA4HUf"
      },
      "source": [
        "encoded_texts = tds\n",
        "encoded_texts_test = test_tds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "mYW7VwC84HUj"
      },
      "source": [
        "# get the unique list of intents' names\n",
        "\n",
        "intents = [d.intent for d in train_data]\n",
        "intent_names = list(set(intents))\n",
        "intent_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqu-VuS44HUk"
      },
      "source": [
        "# map intents' names to indexes, which is going to be used by the model to assign predictions\n",
        "intent_map = dict() # index -> intent\n",
        "for idx, ui in enumerate(intent_names):\n",
        "    intent_map[ui] = idx\n",
        "intent_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse intents' names with ids, to use when predicting for converting an ID into natural language\n",
        "\n",
        "id_to_intent_name = {v: k for k, v in intent_map.items()}"
      ],
      "metadata": {
        "id": "hks-oHZH4HUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2xUH5l_4HUn"
      },
      "source": [
        "true_intents, encoded_intents = encode_intents(intents, intent_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJgbEeuX4HUo"
      },
      "source": [
        "# encode slots\n",
        "slot_names = set()\n",
        "for td in train_data:\n",
        "    slots = td.slots\n",
        "    for slot in slots:\n",
        "        slot_names.add(slot)\n",
        "slot_names = list(slot_names)\n",
        "slot_names.insert(0, \"O\")\n",
        "slot_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i3jO8c84HUp"
      },
      "source": [
        "# map slots' names to indexes, which is going to be used by the model to assign predictions\n",
        "\n",
        "slot_map = dict() # slot -> index\n",
        "for idx, us in enumerate(slot_names):\n",
        "    slot_map[us] = idx\n",
        "slot_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse slots' names with ids, to use when predicting for converting an ID into natural language\n",
        "\n",
        "id_to_slot_name = {v: k for k, v in slot_map.items()}"
      ],
      "metadata": {
        "id": "G6Pup_Nj4HUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test to see if the slots are aligned with the texts, by getting the slot name for a token within a text\n",
        "\n",
        "print(train_data[0].text)\n",
        "print(train_data[0].slots)\n",
        "print(\"slot_name for westbam is : \", get_slot_from_word(\"westbam\", train_data[0].slots))"
      ],
      "metadata": {
        "id": "UhBP4E9hoqFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the max encoded test length\n",
        "# tokenizer pads all texts to same length anyway so\n",
        "# just get the length of the first one's input_ids\n",
        "\n",
        "max_len = len(encoded_texts[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "119ava3ioQ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjbt0brf4HUy"
      },
      "source": [
        "# get all the slots and texts, to encode the slots\n",
        "\n",
        "all_slots = [td.slots for td in train_data]\n",
        "all_texts = [td.text for td in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwFWUwrg4HUz"
      },
      "source": [
        "encoded_slots = encode_slots(all_slots, all_texts, tokenizer, slot_map, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-66ZS9C4HU4"
      },
      "source": [
        "joint_model_snips = JointIntentAndSlotFillingModel(\n",
        "    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-yz2wV4HU6"
      },
      "source": [
        "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
        "\n",
        "# two outputs, one for slots, another for intents\n",
        "# we have to fine tune for both\n",
        "losses = [SparseCategoricalCrossentropy(from_logits=True),\n",
        "          SparseCategoricalCrossentropy(from_logits=True)]\n",
        "\n",
        "metrics = [SparseCategoricalAccuracy(\"accuracy\")]\n",
        "# compile model\n",
        "joint_model_snips.compile(optimizer=opt, loss=losses, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgOEgVGR4HU7"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GjWvsX34HU9"
      },
      "source": [
        "x = {\"input_ids\": encoded_texts[\"input_ids\"], \"token_type_ids\": encoded_texts[\"token_type_ids\"],  \"attention_mask\": encoded_texts[\"attention_mask\"]}\n",
        "\n",
        "history = joint_model_snips.fit(\n",
        "    x, (encoded_slots[:13084], encoded_intents[:13084]), epochs=2, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTWTcNwb4HU9"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slots labels for text below\n",
        "#O B-artist I-artist O O B-playlist I-playlist O"
      ],
      "metadata": {
        "id": "o114Gceu4HU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdbsLrDS4HVA"
      },
      "source": [
        "nlu(\"add sabrina salerno to the grime instrumentals playlist\", tokenizer, joint_model_snips,\n",
        "    intent_names, slot_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slots labels for text below\n",
        "#O O O O B-party_size_number O O O O O O B-spatial_relation O B-poi O O B-restaurant_type O"
      ],
      "metadata": {
        "id": "AzesBYYv4HVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O1HL5oq4HVB"
      },
      "source": [
        "nlu(\"i want to bring four people to a place that s close to downtown that serves churrascaria cuisine\", tokenizer, joint_model_snips,\n",
        "    intent_names, slot_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict intent and slots for texts in the test dataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "slots_ids = []\n",
        "intent_ids = []\n",
        "for i in tqdm(range(len(all_texts[13084:]))):\n",
        "    res, slot_id, intent_id = nlu(all_texts[i+13084], tokenizer, joint_model_snips, slot_names, slot_names)\n",
        "    results.append(res)\n",
        "    slots_ids.append(slot_id)\n",
        "    intent_ids.append(intent_id)"
      ],
      "metadata": {
        "id": "5smPq49j4HVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate slots metrics\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(slots_ids)):\n",
        "    for prediction, label in zip(slots_ids, encoded_slots[13084:]):\n",
        "        for predicted_idx, label_idx in zip(prediction, label):\n",
        "            all_predictions.append(id_to_slot_name[predicted_idx])\n",
        "            all_labels.append(id_to_slot_name[label_idx])\n",
        "metrics_to_write = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write"
      ],
      "metadata": {
        "id": "Gj_LRzMi4HVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate intent metrics\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(intent_ids)):\n",
        "        for predicted_idx, label_idx in zip(intent_ids, true_intents[13084:]):\n",
        "            all_predictions.append(id_to_intent_name[predicted_idx])\n",
        "            all_labels.append(id_to_intent_name[label_idx])\n",
        "metrics_to_write_intent = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write_intent"
      ],
      "metadata": {
        "id": "GZAnp0Kx4HVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#manual slots accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "for prediction, label in zip(slots_ids, encoded_slots[13084:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/counter)"
      ],
      "metadata": {
        "id": "JEWyogBA4HVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual overall accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "final_correct = 0\n",
        "i = 13084\n",
        "j = 0\n",
        "train_set_length = 700\n",
        "for prediction, label in zip(slots_ids, encoded_slots[13084:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "  if counter == correct:\n",
        "    print(i,j)\n",
        "    if intent_ids[j] == true_intents[i]:\n",
        "      final_correct +=1\n",
        "  correct = 0\n",
        "  counter = 0\n",
        "  i += 1\n",
        "  j += 1\n",
        "print(final_correct/train_set_length)"
      ],
      "metadata": {
        "id": "X7YuPoqKGAj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual intent metrics\n",
        "\n",
        "correct = 0\n",
        "for p, l in zip(intent_ids, true_intents[13084:]):\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/train_set_length)"
      ],
      "metadata": {
        "id": "HuAB1aqR4HVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Generated Dataset**"
      ],
      "metadata": {
        "id": "fKtTqetCgJ77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if files do not load via drag and drop, you can use the files.upload function\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "IXsykFA-HXSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k_Qd2JrgQgW"
      },
      "source": [
        "# read from json file\n",
        "# the notebook is set to train for the best generated dataset (as described in the paper) by default. Any change to that requires multiple changes in the continuing code\n",
        "\n",
        "train_data = read_train_json_file(\"train_1.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xy-uqdmQgQgY"
      },
      "source": [
        "example = train_data[0]\n",
        "example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "4h0O0_4ow9zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU0afMvUgQgb"
      },
      "source": [
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "# for the default set (train.json), use 8750 as train limit and 8751 as test starting index.\n",
        "# for the train_625.json set, use 2000 as train limit and 2001 as test starting index.\n",
        "# for the train_1250.json set, use 3800 as train limit and 3801 as test starting index.\n",
        "# for the train_5000.json set, use 16500 as train limit and 16501 as test starting index.\n",
        "\n",
        "limit = 3800\n",
        "test_start_index = 3801\n",
        "\n",
        "def encode_texts(tokenizer, texts):\n",
        "    return tokenizer(texts[:limit], padding=True, truncation=True, return_tensors=\"tf\"), tokenizer(texts[test_start_index:], padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "texts = [d.text for d in train_data]\n",
        "tds, test_tds = encode_texts(tokenizer, texts)\n",
        "tds.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHh_upNAgQgc"
      },
      "source": [
        "encoded_texts = tds\n",
        "encoded_texts_test = test_tds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_texts['input_ids'])"
      ],
      "metadata": {
        "id": "Jr-E9gYOj8U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_texts_test['input_ids'])"
      ],
      "metadata": {
        "id": "1FTuKYFklrM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "_h4SAnj9gQgd"
      },
      "source": [
        "# get the unique list of intents' names\n",
        "\n",
        "intents = [d.intent for d in train_data]\n",
        "intent_names = list(set(intents))\n",
        "intent_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th_Z6p6sgQgd"
      },
      "source": [
        "# map intents' names to indexes, which is going to be used by the model to assign predictions\n",
        "\n",
        "intent_map = dict() # index -> intent\n",
        "for idx, ui in enumerate(intent_names):\n",
        "    intent_map[ui] = idx\n",
        "intent_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse intents' names with ids, to use when predicting for converting an ID into natural language\n",
        "\n",
        "id_to_intent_name = {v: k for k, v in intent_map.items()}"
      ],
      "metadata": {
        "id": "ENtYscdJgQge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD3WeiTegQge"
      },
      "source": [
        "true_intents, encoded_intents = encode_intents(intents, intent_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yevZ7Mi2gQgf"
      },
      "source": [
        "# encode slots\n",
        "slot_names = set()\n",
        "for td in train_data:\n",
        "    slots = td.slots\n",
        "    for slot in slots:\n",
        "        slot_names.add(slot)\n",
        "slot_names = list(slot_names)\n",
        "slot_names.insert(0, \"O\")\n",
        "slot_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OafHc9aVgQgg"
      },
      "source": [
        "# map slots' names to indexes, which is going to be used by the model to assign predictions\n",
        "\n",
        "slot_map = dict() # slot -> index\n",
        "for idx, us in enumerate(slot_names):\n",
        "    slot_map[us] = idx\n",
        "slot_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse slots' names with ids, to use when predicting for converting an ID into natural language\n",
        "\n",
        "id_to_slot_name = {v: k for k, v in slot_map.items()}"
      ],
      "metadata": {
        "id": "P2AnML9QgQgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(slot_map.keys())"
      ],
      "metadata": {
        "id": "alxLYOt5gQgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyBjvr9YgQgh"
      },
      "source": [
        "# find the max encoded test length\n",
        "# tokenizer pads all texts to same length anyway so\n",
        "# just get the length of the first one's input_ids\n",
        "\n",
        "max_len = len(encoded_texts[\"input_ids\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIWblryvgQgi"
      },
      "source": [
        "# get all slots and texts, to encode slots\n",
        "\n",
        "all_slots = [td.slots for td in train_data]\n",
        "all_texts = [td.text for td in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AshxU8pgQgi"
      },
      "source": [
        "encoded_slots = encode_slots(all_slots, all_texts, tokenizer, slot_map, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_texts_test['input_ids'])"
      ],
      "metadata": {
        "id": "zp38G137PpME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yy11FT4gQgj"
      },
      "source": [
        "joint_model = JointIntentAndSlotFillingModel(\n",
        "    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CtikOpIgQgk"
      },
      "source": [
        "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
        "\n",
        "# two outputs, one for slots, another for intents\n",
        "# we have to fine tune for both\n",
        "losses = [SparseCategoricalCrossentropy(from_logits=True),\n",
        "          SparseCategoricalCrossentropy(from_logits=True)]\n",
        "\n",
        "metrics = [SparseCategoricalAccuracy(\"accuracy\")]\n",
        "# compile model\n",
        "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_texts_test['input_ids'])"
      ],
      "metadata": {
        "id": "cAI40jPkgQgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_texts['input_ids'])"
      ],
      "metadata": {
        "id": "ssmxYQqCgQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-hxuy-_gQgl"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOCNd7d4gQgm"
      },
      "source": [
        "x = {\"input_ids\": encoded_texts[\"input_ids\"], \"token_type_ids\": encoded_texts[\"token_type_ids\"],  \"attention_mask\": encoded_texts[\"attention_mask\"]}\n",
        "\n",
        "history = joint_model.fit(\n",
        "    x, (encoded_slots[:limit], encoded_intents[:limit]), validation_split = 0.2, epochs=2, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psCcTJDHgQgm"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joint_model.summary()"
      ],
      "metadata": {
        "id": "10q_VnEXIKEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwdpBfAhgQgn"
      },
      "source": [
        "# intent and slots (with their positions), for the text below\n",
        "#\n",
        "#        \"slots\": {\n",
        "#            \"B-hasName\": \"Pitch-Footballer\"\n",
        "#        },\n",
        "#        \"positions\": {\n",
        "#            \"B-hasName\": [\n",
        "#               16,\n",
        "#               31\n",
        "#            ]\n",
        "#        },\n",
        "#        \"intent\": \"insert\"\n",
        "\n",
        "nlu(\"(agree) name is Pitch-Footballer\", tokenizer, joint_model,\n",
        "    intent_names, slot_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict intent and slots for each text in the test dataset\n",
        "\n",
        "results = []\n",
        "slots_ids = []\n",
        "intent_ids = []\n",
        "for i in tqdm(range(len(all_texts[test_start_index:]))):\n",
        "    res, slot_id, intent_id = nlu(all_texts[i+test_start_index], tokenizer, joint_model, slot_names, slot_names)\n",
        "    results.append(res)\n",
        "    slots_ids.append(slot_id)\n",
        "    intent_ids.append(intent_id)"
      ],
      "metadata": {
        "id": "r7kxR5hAgQgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(slots_ids)"
      ],
      "metadata": {
        "id": "YKDFaIHkgQgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_texts)"
      ],
      "metadata": {
        "id": "PVAAqqlKs3_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate slots metrics\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(slots_ids)):\n",
        "    for prediction, label in zip(slots_ids, encoded_slots[test_start_index:]):\n",
        "        for predicted_idx, label_idx in zip(prediction, label):\n",
        "            all_predictions.append(id_to_slot_name[predicted_idx])\n",
        "            all_labels.append(id_to_slot_name[label_idx])\n",
        "metrics_to_write = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write"
      ],
      "metadata": {
        "id": "FncbaTtygQgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate intent metrics\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for i in range(len(intent_ids)):\n",
        "        for predicted_idx, label_idx in zip(intent_ids, true_intents[test_start_index:]):\n",
        "            all_predictions.append(id_to_intent_name[predicted_idx])\n",
        "            all_labels.append(id_to_intent_name[label_idx])\n",
        "metrics_to_write_intent = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
        "metrics_to_write_intent"
      ],
      "metadata": {
        "id": "aQas-pEsgQgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually calculated slot accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "for prediction, label in zip(slots_ids, encoded_slots[test_start_index:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/counter)"
      ],
      "metadata": {
        "id": "8ATjHjKpgQgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually calculated overall accuracy\n",
        "\n",
        "correct = 0\n",
        "counter = 0\n",
        "final_correct = 0\n",
        "i = test_start_index\n",
        "j = 0\n",
        "for prediction, label in zip(slots_ids, encoded_slots[test_start_index:]):\n",
        "  for p, l in zip(prediction, label):\n",
        "    counter += 1\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "  if counter == correct:\n",
        "    print(i,j)\n",
        "    if intent_ids[j] == true_intents[i]:\n",
        "      final_correct +=1\n",
        "  correct = 0\n",
        "  counter = 0\n",
        "  i += 1\n",
        "  j += 1\n",
        "print(final_correct/len(encoded_slots[test_start_index:]))"
      ],
      "metadata": {
        "id": "d6udfIzWgQgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually calculated intent accuracy\n",
        "\n",
        "correct = 0\n",
        "for p, l in zip(intent_ids, true_intents[test_start_index:]):\n",
        "    if p == l:\n",
        "      correct += 1\n",
        "print(correct/len(encoded_slots[test_start_index:]))"
      ],
      "metadata": {
        "id": "c9nLigtEgQgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}